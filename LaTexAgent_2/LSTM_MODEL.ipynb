{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Notebook For LSTM model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing and extracting training file and model.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    \"https://github.com/syedhamzamohiuddin/Final_SE/tree/main/LaTexAgent_2/my_model.tar.xz\" \\\n",
    "    -O \"/content/my_model.tar.xz\"\n",
    "!tar xf my_model.tar.xz\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    \"https://github.com/syedhamzamohiuddin/FInal_SE/tree/main/LaTexAgent_2/training_data.tar.xz\" \\\n",
    "    -O \"/content/training_data.tar.xz\"\n",
    "!tar xf training_data.tar.xz\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    \"https://github.com/syedhamzamohiuddin/FInal_SE/tree/main/LaTexAgent_2/training_data.tar.xz\" \\\n",
    "    -O \"/content/pdfs.tar.xz\"\n",
    "!tar xf pdfs.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading the training file</h3>\n",
    "<p>This cell does the following:<br>1,Reads the training file named, \"train.txt\".<br>2,Replaces newline characters with spaces.<br>3,Creates a vocabulary of characters present in the corpus, and then prints them.<p>\n",
    "    \n",
    "<h4>Explanation:</h4>\n",
    "<p>This is a character based model, <b>not</b> a word-based, so we replace the new-lines with space <br>as new lines don't matter because the recognition is being done at character level.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 76436\n",
      "Total chars: 93\n",
      "  ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\ ] ^ _ a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ~ "
     ]
    }
   ],
   "source": [
    "with io.open(\"training_data/train/train.txt\") as f:\n",
    "    text = f.read()\n",
    "text = text.replace(\"\\n\",\" \")\n",
    "print(\"Corpus length:\", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print(\"Total chars:\", len(chars))\n",
    "\n",
    "for c in chars:\n",
    "    print(c, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating mappings using python dictionaries:</h3>\n",
    "<p>In the first line, a python dictionary that maps characters to index is being created.<br>In the second line, a python dictionary that maps index to characters is being created.</p>\n",
    "\n",
    "<h4>Explanation:</h4>\n",
    "<p>As neural networks work with numbers, char_to_ix dictionary is used.<br>Neural network outputs the probabilites for each index. To map the index chosen in the 'sample' method, ix_to_char dictionary is used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating Training sequences:</h3>\n",
    "<p>Note that the variable \"text\" contains the whole training data with no new lines.<br>Read the comments in the cell below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 25476\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10       #Length of input sequence. It means that LSTM will take 10 characters as input at a time.\n",
    "step = 3          #It means that after taking first 10 characters (0-9) for first training example,the second training example\n",
    "                  #would have charaters from 3-12, fourth will have characters from 6-15 and so on.\n",
    "sentences = []    #These are the Training examples\n",
    "next_chars = []   #These are the output for each training example. For example,Given first ten characters, what's the eleventh character?\n",
    "\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):#Each iteration is incremented by step size, whch is 3.\n",
    "    sentences.append(text[i : i + maxlen])   #Training example being added to 'sentences'. Notice it stores the example\n",
    "                                             #That starts at i and ends at i+10.\n",
    "    next_chars.append(text[i + maxlen])      #This is the output (the 11th character) for the corresponding training example (consecutive 10 characters).\n",
    "print(\"Number of sequences:\", len(sentences))\n",
    "\n",
    "#Input\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)  #Empty boolean array of dimensions len(sentences) by maxlen by len(chars),where\n",
    "#output                                                            # for each character, its corresponding position is filled with 1,the rest with zeros.\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)          #Empty boolean array of dimensions (len(sentences), len(chars))\n",
    "\n",
    "\n",
    "#The above empty arrays are filled with 1s.\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_ix[char]] = 1    # character is mapped to index for input array\n",
    "    y[i, char_to_ix[next_chars[i]]] = 1  # character is mapped to index for output array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(maxlen, len(chars))),  #Input layer that takes input of shape (maxlen, len(chars)), which is the shape of a training example.\n",
    "        keras.layers.LSTM(128),                   #LSTM layer\n",
    "        keras.layers.Dense(len(chars), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)  #optimizer\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer) #loss fucntion that will be optimized using optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sample Function:</h3>\n",
    "<p>This function takes the output from the LSTM, which is the output of softmax of dimension len(chars).<br>\n",
    "It then creates a probability distribution of all the values in the output array 'preds'<br>\n",
    "Finally, it appliex the argmax function and gets the index.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training the model</h3>\n",
    "<p>The model is trained 'epochs'times.<br>Each epoch loads the data in batch sizes of 'batch_size'</p>\n",
    "<p>You can vary these parameters<p>\n",
    "<h4>Note:</h4> Only run the cell below if you want to train and save the model yourself. Skip otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 12s 59ms/step - loss: 2.4518\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 1.7027\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 1.4197\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.2187\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.0634\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.9365\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.8319\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.7591\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.6789\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.6250\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.5811\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.5393\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.5030\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.4882\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.4552\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.4393\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.4255\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.4034\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.3963\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.3869\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.3759\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.3621\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.3499\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.3525\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.3464\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.3366\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 0.3338\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.3215\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.3155\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 0.3142\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.3113\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.3066\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.3006\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2958\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2947\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2951\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2886\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.2857\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2841\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2812\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2805\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.2701\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.2712\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2708\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.2701\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2624\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2653\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.2674\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2623\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.2573\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2588\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2579\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2566\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2549\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2560\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2487\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2486\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2474\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2440\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2464\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2430\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2415\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2393\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2384\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2367\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2358\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2334\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2295\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2334\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2303\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.2306\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2262\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2280\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2207\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2245\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2264\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2244\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2216\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2244\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2203\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2162\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2197\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.2238\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2216\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.2200\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2168\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.2166\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2168\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2149\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2134\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2150\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2134\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2129\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2152\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.2132\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.2101\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2114\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2101\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2128\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2072\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2099\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.2093\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2091\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.2033\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2058\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2082\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.1992\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2060\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2042\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2058\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2030\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2048\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2032\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2016\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2002\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2018\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1994\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1994\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.2011\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.2022\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1996\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1984\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1970\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1988\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1976\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1980\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1989\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1947\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1945\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1939\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1930\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.1964\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.1961\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1922\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1922\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1916\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1911\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1915\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1920\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1915\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1895\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1952\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1904\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1879\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1893\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1889\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1885\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 0.1894\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1892\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1874\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1903\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1867\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1918\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1906\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1876\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1896\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1853\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1865\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1858\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1852\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1885\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1861\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1857\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1843\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1853\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1879\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1855\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1852\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1843\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1856\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1811\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1846\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1859\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1818\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1815\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1821\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1821\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.1830\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1838\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1848\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1821\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1838\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1802\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1808\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1822\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1794\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1787\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1814\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1854\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1814\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1819\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1785\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1808\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1808\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1816\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 0.1793\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1775\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1778\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1822\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.1781\n",
      "WARNING:tensorflow:From /home/syed/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "\n",
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get predictions given input:</h3>\n",
    "<p>It generates next 20 characters, given 10 characters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictt(inpt):\n",
    "    \n",
    "    generated = \"\"\n",
    "    sentence = inpt\n",
    "    print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "    for i in range(20):      #amount of predictions\n",
    "        x_pred = np.zeros((1, maxlen, len(chars))) #Empty array for the prediction\n",
    "        for t, char in enumerate(sentence):        #For each character in the sequence, set the value to 1.0 for each character's corresponding index.\n",
    "            x_pred[0, t, char_to_ix[char]] = 1.0\n",
    "            \n",
    "        preds = reconstructed_model.predict(x_pred, verbose=0)[0] # Get output from the model.\n",
    "        next_index = sample(preds, 0.2)                           # Create sample distribution, and get the index from that distribution.\n",
    "        next_char = ix_to_char[next_index]\n",
    "        sentence = sentence[1:] + next_char                       #Shift the sentence one step forward by dropping the first character and adding the\n",
    "                                                                  #predicted character at the end.\n",
    "       # print(sentence)\n",
    "        if next_char != '\\n':\n",
    "            generated += next_char                                #add the predicted chracter to the list of generated to print.\n",
    "\n",
    "  \n",
    "    print(inpt+generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading a saved model:</h3>\n",
    "\n",
    "<p>Remember that \"model_name\" here is the name of the folder that tensorflow created when I called<br>\n",
    "    model.save(\"model_name\") method. So just pass the path to this folder,<b>not</b> to the contents inside of it.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predict!!</h3>\n",
    "<p>Enter 10 character string, and don't forget to escape special characters with backslash n.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Generating with seed: \"\\frac{1}{\\\"\n",
      "\\frac{1}{\\, } { 1 - \\e ^ { v_i\n"
     ]
    }
   ],
   "source": [
    "predictt(\"\\\\frac{1}{\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
